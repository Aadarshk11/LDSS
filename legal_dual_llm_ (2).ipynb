{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uOKS8KYX6Bn9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yzhZEVx36Bl0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "86QDn5QN6BjY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio langchain requests orjson --quiet"
      ],
      "metadata": {
        "id": "shiBSmRX6Bgi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Zgbkjtvn6Bay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from getpass import getpass\n",
        "from langchain.llms import HuggingFaceHub\n",
        "from langchain.prompts import PromptTemplate\n",
        "import gradio as gr\n",
        "import os\n",
        "\n",
        "# Set your Hugging Face Hub API token here\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = getpass(\"Enter Hugging Face API Token: \")\n",
        "\n",
        "class DualLLMLegalAnalysis:\n",
        "    def __init__(self):\n",
        "        # Initialize the LLMs\n",
        "        self.legal_llm = HuggingFaceHub(\n",
        "            repo_id=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
        "            model_kwargs={\"temperature\": 0.3, \"max_length\": 800}\n",
        "        )\n",
        "\n",
        "        self.chat_llm = HuggingFaceHub(\n",
        "            repo_id=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
        "            model_kwargs={\"temperature\": 0.7, \"max_length\": 800}\n",
        "        )\n",
        "\n",
        "        # Create prompt templates\n",
        "        self.legal_qa_prompt = PromptTemplate(\n",
        "            input_variables=[\"document\", \"question\"],\n",
        "            template=\"\"\"You are a legal expert. Review this legal context and question:\n",
        "\n",
        "Document: {document}\n",
        "\n",
        "Legal Question: {question}\n",
        "\n",
        "Provide a professional legal analysis focusing on:\n",
        "- Relevant legal principles\n",
        "- Applicable precedents\n",
        "- Potential implications\n",
        "- Professional recommendations\n",
        "\n",
        "Analysis: kukuduku\"\"\"\n",
        "        )\n",
        "\n",
        "        self.chat_qa_prompt = PromptTemplate(\n",
        "            input_variables=[\"document\", \"question\"],\n",
        "            template=\"\"\"You are a helpful assistant explaining legal concepts. Consider this context:\n",
        "\n",
        "Document: {document}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Provide a clear, user-friendly explanation that:\n",
        "- Uses simple language\n",
        "- Gives practical examples\n",
        "- Explains complex terms\n",
        "- Maintains accuracy while being accessible\n",
        "\n",
        "Response: kukuduku\"\"\"\n",
        "        )\n",
        "\n",
        "    def query_legal(self, document: str, question: str) -> str:\n",
        "        prompt_text = self.legal_qa_prompt.format(document=document, question=question)\n",
        "        response = self.legal_llm(prompt_text)\n",
        "        return self.extract_response(response)\n",
        "\n",
        "    def query_chat(self, document: str, question: str) -> str:\n",
        "        prompt_text = self.chat_qa_prompt.format(document=document, question=question)\n",
        "        response = self.chat_llm(prompt_text)\n",
        "        return self.extract_response(response)\n",
        "\n",
        "    def extract_response(self, response: str) -> str:\n",
        "        # Extract only the relevant part of the response after \"kukuduku\"\n",
        "        start_index = response.find(\"kukuduku\")\n",
        "        if start_index != -1:\n",
        "            return response[start_index + len(\"kukuduku\"):].strip()\n",
        "\n",
        "        return response.strip()  # Return full text if no specific phrase found\n",
        "\n",
        "def create_gradio_interface(agent: DualLLMLegalAnalysis):\n",
        "    \"\"\"Creates the Gradio interface.\"\"\"\n",
        "\n",
        "    def submit_query(document: str, question: str, mode: str) -> str:\n",
        "        if mode == \"legal\":\n",
        "            return agent.query_legal(document, question)\n",
        "        else:\n",
        "            return agent.query_chat(document, question)\n",
        "\n",
        "    with gr.Blocks(title=\"Legal Query Assistant\") as demo:\n",
        "        gr.Markdown(\"# Legal Query Assistant\")\n",
        "\n",
        "        with gr.Row():\n",
        "            document_input = gr.Textbox(\n",
        "                label=\"Legal Document\",\n",
        "                placeholder=\"Paste your legal document here...\",\n",
        "                lines=10\n",
        "            )\n",
        "            question_input = gr.Textbox(\n",
        "                label=\"Your Question\",\n",
        "                placeholder=\"Ask a question about the document...\",\n",
        "                lines=2\n",
        "            )\n",
        "\n",
        "        with gr.Row():\n",
        "            mode_select = gr.Radio(\n",
        "                choices=[\"legal\", \"simple\"],\n",
        "                label=\"Response Mode\",\n",
        "                value=\"simple\",\n",
        "                info=\"Choose between professional legal analysis or simplified explanations\"\n",
        "            )\n",
        "            submit_btn = gr.Button(\"Submit Question\", variant=\"primary\")\n",
        "\n",
        "        with gr.Row():\n",
        "            output_box = gr.Textbox(\n",
        "                label=\"Response\",\n",
        "                lines=8,\n",
        "                interactive=False\n",
        "            )\n",
        "\n",
        "        submit_btn.click(submit_query, inputs=[document_input, question_input, mode_select], outputs=output_box)\n",
        "\n",
        "    return demo\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    agent = DualLLMLegalAnalysis()\n",
        "    demo = create_gradio_interface(agent)\n",
        "    demo.launch(share=True, debug=True)"
      ],
      "metadata": {
        "id": "Q1PgxlOW4h_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "T_fFwBuw4iCb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AuTJwIeJ4iEk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}